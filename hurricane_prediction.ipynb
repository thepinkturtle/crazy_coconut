{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# one hot encode the dataset, only take 6:00, 12:00 and 18:00 b/c that when most \n",
    "# of the times are taken\n",
    "dataframe = pd.read_csv(\"atlantic_without_NW.csv\")\n",
    "dataframe[\"time\"] = dataframe[\"time\"].astype('category')\n",
    "dataframe['date'] = [str(i)[4:6] for i in dataframe['date']] # Get just the month\n",
    "\n",
    "dataframe = pd.get_dummies(dataframe, columns=[\"time\",\"rating\",\"date\"])\n",
    "dataframe.drop([\"ocean\",\"name\"],axis=1, inplace=True)\n",
    "\n",
    "y = dataframe[[\"rating_ DB\",\"rating_ EX\",\"rating_ HU\",\"rating_ LO\",\"rating_ SD\",\"rating_ SS\",\"rating_ TD\",\"rating_ TS\",\"rating_ V\"]].values\n",
    "\n",
    "X = dataframe[[\"lat\",\"long\",\"wind_speed\",\"pressure\",\"time_600\",\"time_1200\",\"time_1800\",\"date_01\",\"date_04\",\"date_05\",\"date_06\",\"date_07\",\"date_08\",\"date_09\",\"date_10\",\"date_11\",\"date_12\"]].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# delete all the rows that contain no readings for pressure\n",
    "rows_to_delete = []\n",
    "row_num = 0\n",
    "\n",
    "for row in X:\n",
    "    if row[3] == -999:\n",
    "        rows_to_delete.append(row_num)\n",
    "    row_num += 1\n",
    "\n",
    "X = np.delete(X, rows_to_delete, 0)        # remove all rows with bad data\n",
    "y = np.delete(y, rows_to_delete, 0)\n",
    "\n",
    "encoded_lat_long = np.zeros(shape=(len(X),3))\n",
    "count = 0\n",
    "R = 6371 # radius of earth\n",
    "for row in X:\n",
    "    lat = np.deg2rad(row[0])\n",
    "    lng = np.deg2rad(row[1])\n",
    "\n",
    "    encoded_lat_long[count][0] = R * (math.cos(lat) * math.cos(lng))\n",
    "    encoded_lat_long[count][1] = R * (math.cos(lat) * math.sin(lng))\n",
    "    encoded_lat_long[count][2] = R * math.sin(lat)\n",
    "    count += 1\n",
    "\n",
    "X = np.delete(X, 0, 1)                     # remove old lat\n",
    "X = np.delete(X, 0, 1)                     # remove old long\n",
    "X = np.append(X, encoded_lat_long, axis= 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[2.50000000e+01 1.00900000e+03 0.00000000e+00 1.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 3.46581665e+03\n 4.98665664e+03 1.92639857e+03]\n"
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataset into training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()\n",
    "\n",
    "# input layer\n",
    "classifier.add(Dense(output_dim=9, init='uniform', activation='relu', input_dim=18))\n",
    "\n",
    "# hidden layer\n",
    "classifier.add(Dense(output_dim=10, init='uniform', activation='relu'))\n",
    "\n",
    "# hidden layer\n",
    "classifier.add(Dense(output_dim=14, init='uniform', activation='relu'))\n",
    "\n",
    "# hidden layer\n",
    "classifier.add(Dense(output_dim=16, init='uniform', activation='relu'))\n",
    "\n",
    "# output layer\n",
    "classifier.add(Dense(output_dim=9, init='uniform', activation='softmax'))\n",
    "\n",
    "# create the NN\n",
    "classifier.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "=====] - 3s 279us/step - loss: 0.6673 - accuracy: 0.7809 - val_loss: 0.6136 - val_accuracy: 0.8000\nEpoch 69/200\n12380/12380 [==============================] - 3s 274us/step - loss: 0.6687 - accuracy: 0.7809 - val_loss: 0.6124 - val_accuracy: 0.8000\nEpoch 70/200\n12380/12380 [==============================] - 3s 270us/step - loss: 0.6629 - accuracy: 0.7811 - val_loss: 0.6101 - val_accuracy: 0.8000\nEpoch 71/200\n12380/12380 [==============================] - 3s 259us/step - loss: 0.6652 - accuracy: 0.7814 - val_loss: 0.6109 - val_accuracy: 0.8000\nEpoch 72/200\n12380/12380 [==============================] - 3s 264us/step - loss: 0.6620 - accuracy: 0.7821 - val_loss: 0.6188 - val_accuracy: 0.8000\nEpoch 73/200\n12380/12380 [==============================] - 3s 272us/step - loss: 0.6722 - accuracy: 0.7808 - val_loss: 0.6093 - val_accuracy: 0.7994\nEpoch 74/200\n12380/12380 [==============================] - 3s 266us/step - loss: 0.6585 - accuracy: 0.7829 - val_loss: 0.6143 - val_accuracy: 0.8000\nEpoch 75/200\n12380/12380 [==============================] - 3s 272us/step - loss: 0.6608 - accuracy: 0.7821 - val_loss: 0.6265 - val_accuracy: 0.7984\nEpoch 76/200\n12380/12380 [==============================] - 3s 266us/step - loss: 0.6663 - accuracy: 0.7820 - val_loss: 0.6103 - val_accuracy: 0.8000\nEpoch 77/200\n12380/12380 [==============================] - 3s 269us/step - loss: 0.6702 - accuracy: 0.7811 - val_loss: 0.6108 - val_accuracy: 0.8000\nEpoch 78/200\n12380/12380 [==============================] - 3s 262us/step - loss: 0.6594 - accuracy: 0.7830 - val_loss: 0.6172 - val_accuracy: 0.7997\nEpoch 79/200\n12380/12380 [==============================] - 3s 271us/step - loss: 0.6631 - accuracy: 0.7817 - val_loss: 0.6096 - val_accuracy: 0.8000\nEpoch 80/200\n12380/12380 [==============================] - 3s 267us/step - loss: 0.6631 - accuracy: 0.7813 - val_loss: 0.6129 - val_accuracy: 0.8000\nEpoch 81/200\n12380/12380 [==============================] - 3s 263us/step - loss: 0.6601 - accuracy: 0.7829 - val_loss: 0.6128 - val_accuracy: 0.8000\nEpoch 82/200\n12380/12380 [==============================] - 4s 291us/step - loss: 0.6645 - accuracy: 0.7817 - val_loss: 0.6229 - val_accuracy: 0.7990\nEpoch 83/200\n12380/12380 [==============================] - 3s 263us/step - loss: 0.6617 - accuracy: 0.7826 - val_loss: 0.6094 - val_accuracy: 0.8000\nEpoch 84/200\n12380/12380 [==============================] - 3s 264us/step - loss: 0.6608 - accuracy: 0.7820 - val_loss: 0.6143 - val_accuracy: 0.8000\nEpoch 85/200\n12380/12380 [==============================] - 3s 278us/step - loss: 0.6591 - accuracy: 0.7829 - val_loss: 0.6092 - val_accuracy: 0.8000\nEpoch 86/200\n12380/12380 [==============================] - 3s 258us/step - loss: 0.6602 - accuracy: 0.7830 - val_loss: 0.6106 - val_accuracy: 0.8000\nEpoch 87/200\n12380/12380 [==============================] - 3s 272us/step - loss: 0.6643 - accuracy: 0.7811 - val_loss: 0.6119 - val_accuracy: 0.8000\nEpoch 88/200\n12380/12380 [==============================] - 3s 267us/step - loss: 0.6574 - accuracy: 0.7831 - val_loss: 0.6102 - val_accuracy: 0.8000\nEpoch 89/200\n12380/12380 [==============================] - 3s 264us/step - loss: 0.6639 - accuracy: 0.7813 - val_loss: 0.6155 - val_accuracy: 0.8000\nEpoch 90/200\n12380/12380 [==============================] - 3s 269us/step - loss: 0.6613 - accuracy: 0.7827 - val_loss: 0.6131 - val_accuracy: 0.8000\nEpoch 91/200\n12380/12380 [==============================] - 3s 263us/step - loss: 0.6627 - accuracy: 0.7817 - val_loss: 0.6139 - val_accuracy: 0.8000\nEpoch 92/200\n12380/12380 [==============================] - 3s 262us/step - loss: 0.6600 - accuracy: 0.7823 - val_loss: 0.6122 - val_accuracy: 0.8000\nEpoch 93/200\n12380/12380 [==============================] - 3s 259us/step - loss: 0.6576 - accuracy: 0.7830 - val_loss: 0.6137 - val_accuracy: 0.8000\nEpoch 94/200\n12380/12380 [==============================] - 3s 264us/step - loss: 0.6606 - accuracy: 0.7826 - val_loss: 0.6128 - val_accuracy: 0.8000\nEpoch 95/200\n12380/12380 [==============================] - 3s 266us/step - loss: 0.6579 - accuracy: 0.7831 - val_loss: 0.6172 - val_accuracy: 0.8000\nEpoch 96/200\n12380/12380 [==============================] - 3s 270us/step - loss: 0.6620 - accuracy: 0.7822 - val_loss: 0.6117 - val_accuracy: 0.8000\nEpoch 97/200\n12380/12380 [==============================] - 3s 271us/step - loss: 0.6602 - accuracy: 0.7821 - val_loss: 0.6108 - val_accuracy: 0.8000\nEpoch 98/200\n12380/12380 [==============================] - 3s 269us/step - loss: 0.6577 - accuracy: 0.7826 - val_loss: 0.6144 - val_accuracy: 0.8000\nEpoch 99/200\n12380/12380 [==============================] - 3s 269us/step - loss: 0.6632 - accuracy: 0.7817 - val_loss: 0.6092 - val_accuracy: 0.8000\nEpoch 100/200\n12380/12380 [==============================] - 4s 289us/step - loss: 0.6576 - accuracy: 0.7834 - val_loss: 0.6083 - val_accuracy: 0.8000\nEpoch 101/200\n12380/12380 [==============================] - 3s 258us/step - loss: 0.6630 - accuracy: 0.7821 - val_loss: 0.6273 - val_accuracy: 0.7987\nEpoch 102/200\n12380/12380 [==============================] - 3s 266us/step - loss: 0.6578 - accuracy: 0.7831 - val_loss: 0.6065 - val_accuracy: 0.8000\nEpoch 103/200\n12380/12380 [==============================] - 3s 266us/step - loss: 0.6609 - accuracy: 0.7819 - val_loss: 0.6079 - val_accuracy: 0.8000\nEpoch 104/200\n12380/12380 [==============================] - 3s 266us/step - loss: 0.6585 - accuracy: 0.7831 - val_loss: 0.6119 - val_accuracy: 0.8000\nEpoch 105/200\n12380/12380 [==============================] - 3s 269us/step - loss: 0.6600 - accuracy: 0.7819 - val_loss: 0.6123 - val_accuracy: 0.8000\nEpoch 106/200\n12380/12380 [==============================] - 3s 262us/step - loss: 0.6606 - accuracy: 0.7824 - val_loss: 0.6201 - val_accuracy: 0.7987\nEpoch 107/200\n12380/12380 [==============================] - 3s 279us/step - loss: 0.6721 - accuracy: 0.7796 - val_loss: 0.6156 - val_accuracy: 0.8000\nEpoch 108/200\n12380/12380 [==============================] - 3s 266us/step - loss: 0.6578 - accuracy: 0.7833 - val_loss: 0.6104 - val_accuracy: 0.8000\nEpoch 109/200\n12380/12380 [==============================] - 3s 266us/step - loss: 0.6609 - accuracy: 0.7820 - val_loss: 0.6099 - val_accuracy: 0.8000\nEpoch 110/200\n12380/12380 [==============================] - 3s 272us/step - loss: 0.6686 - accuracy: 0.7809 - val_loss: 0.6111 - val_accuracy: 0.8000\nEpoch 111/200\n12380/12380 [==============================] - 3s 265us/step - loss: 0.6571 - accuracy: 0.7830 - val_loss: 0.6145 - val_accuracy: 0.7994\nEpoch 112/200\n12380/12380 [==============================] - 4s 298us/step - loss: 0.6632 - accuracy: 0.7818 - val_loss: 0.6202 - val_accuracy: 0.8000\nEpoch 113/200\n12380/12380 [==============================] - 3s 268us/step - loss: 0.6587 - accuracy: 0.7823 - val_loss: 0.6097 - val_accuracy: 0.8000\nEpoch 114/200\n12380/12380 [==============================] - 3s 270us/step - loss: 0.6588 - accuracy: 0.7824 - val_loss: 0.6169 - val_accuracy: 0.7990\nEpoch 115/200\n12380/12380 [==============================] - 3s 262us/step - loss: 0.6582 - accuracy: 0.7830 - val_loss: 0.6127 - val_accuracy: 0.8000\nEpoch 116/200\n12380/12380 [==============================] - 3s 263us/step - loss: 0.6579 - accuracy: 0.7834 - val_loss: 0.6082 - val_accuracy: 0.8000\nEpoch 117/200\n12380/12380 [==============================] - 3s 267us/step - loss: 0.6582 - accuracy: 0.7821 - val_loss: 0.6077 - val_accuracy: 0.8000\nEpoch 118/200\n12380/12380 [==============================] - 3s 277us/step - loss: 0.6591 - accuracy: 0.7821 - val_loss: 0.6311 - val_accuracy: 0.7984\nEpoch 119/200\n12380/12380 [==============================] - 3s 267us/step - loss: 0.6604 - accuracy: 0.7823 - val_loss: 0.6081 - val_accuracy: 0.8000\nEpoch 120/200\n12380/12380 [==============================] - 4s 288us/step - loss: 0.6617 - accuracy: 0.7822 - val_loss: 0.6075 - val_accuracy: 0.8000\nEpoch 121/200\n12380/12380 [==============================] - 3s 275us/step - loss: 0.6622 - accuracy: 0.7818 - val_loss: 0.7432 - val_accuracy: 0.7483\nEpoch 122/200\n12380/12380 [==============================] - 3s 265us/step - loss: 0.6588 - accuracy: 0.7824 - val_loss: 0.6085 - val_accuracy: 0.8000\nEpoch 123/200\n12380/12380 [==============================] - 3s 273us/step - loss: 0.6631 - accuracy: 0.7805 - val_loss: 0.6083 - val_accuracy: 0.8000\nEpoch 124/200\n12380/12380 [==============================] - 3s 264us/step - loss: 0.6555 - accuracy: 0.7832 - val_loss: 0.7578 - val_accuracy: 0.7564\nEpoch 125/200\n12380/12380 [==============================] - 3s 275us/step - loss: 0.6628 - accuracy: 0.7806 - val_loss: 0.6096 - val_accuracy: 0.8000\nEpoch 126/200\n12380/12380 [==============================] - 3s 274us/step - loss: 0.6579 - accuracy: 0.7830 - val_loss: 0.6101 - val_accuracy: 0.8000\nEpoch 127/200\n12380/12380 [==============================] - 3s 271us/step - loss: 0.6600 - accuracy: 0.7825 - val_loss: 0.6221 - val_accuracy: 0.7981\nEpoch 128/200\n12380/12380 [==============================] - 3s 271us/step - loss: 0.6640 - accuracy: 0.7809 - val_loss: 0.6136 - val_accuracy: 0.8000\nEpoch 129/200\n12380/12380 [==============================] - 3s 265us/step - loss: 0.6612 - accuracy: 0.7833 - val_loss: 0.6217 - val_accuracy: 0.8000\nEpoch 130/200\n12380/12380 [==============================] - 4s 305us/step - loss: 0.6618 - accuracy: 0.7826 - val_loss: 0.6127 - val_accuracy: 0.8000\nEpoch 131/200\n12380/12380 [==============================] - 3s 282us/step - loss: 0.6564 - accuracy: 0.7830 - val_loss: 0.6112 - val_accuracy: 0.8000\nEpoch 132/200\n12380/12380 [==============================] - 3s 265us/step - loss: 0.6778 - accuracy: 0.7780 - val_loss: 0.6188 - val_accuracy: 0.8000\nEpoch 133/200\n12380/12380 [==============================] - 3s 278us/step - loss: 0.6591 - accuracy: 0.7825 - val_loss: 0.6138 - val_accuracy: 0.8000\nEpoch 134/200\n12380/12380 [==============================] - 3s 267us/step - loss: 0.6599 - accuracy: 0.7821 - val_loss: 0.6139 - val_accuracy: 0.8000\nEpoch 135/200\n12380/12380 [==============================] - 4s 291us/step - loss: 0.6637 - accuracy: 0.7813 - val_loss: 0.6101 - val_accuracy: 0.8000\nEpoch 136/200\n12380/12380 [==============================] - 4s 298us/step - loss: 0.6609 - accuracy: 0.7825 - val_loss: 0.6104 - val_accuracy: 0.8000\nEpoch 137/200\n12380/12380 [==============================] - 3s 264us/step - loss: 0.6566 - accuracy: 0.7832 - val_loss: 0.6199 - val_accuracy: 0.7997\nEpoch 138/200\n12380/12380 [==============================] - 3s 268us/step - loss: 0.6649 - accuracy: 0.7808 - val_loss: 0.6103 - val_accuracy: 0.8000\nEpoch 139/200\n12380/12380 [==============================] - 3s 273us/step - loss: 0.6573 - accuracy: 0.7829 - val_loss: 0.6117 - val_accuracy: 0.8000\nEpoch 140/200\n12380/12380 [==============================] - 3s 273us/step - loss: 0.6590 - accuracy: 0.7821 - val_loss: 0.6105 - val_accuracy: 0.8000\nEpoch 141/200\n12380/12380 [==============================] - 4s 285us/step - loss: 0.6575 - accuracy: 0.7826 - val_loss: 0.6060 - val_accuracy: 0.8000\nEpoch 142/200\n12380/12380 [==============================] - 3s 273us/step - loss: 0.6620 - accuracy: 0.7816 - val_loss: 0.6102 - val_accuracy: 0.8000\nEpoch 143/200\n12380/12380 [==============================] - 3s 262us/step - loss: 0.6599 - accuracy: 0.7820 - val_loss: 0.6109 - val_accuracy: 0.8000\nEpoch 144/200\n12380/12380 [==============================] - 3s 268us/step - loss: 0.6664 - accuracy: 0.7806 - val_loss: 0.6093 - val_accuracy: 0.8000\nEpoch 145/200\n12380/12380 [==============================] - 3s 265us/step - loss: 0.6574 - accuracy: 0.7829 - val_loss: 0.6076 - val_accuracy: 0.8000\nEpoch 146/200\n12380/12380 [==============================] - 3s 277us/step - loss: 0.6632 - accuracy: 0.7809 - val_loss: 0.6100 - val_accuracy: 0.8000\nEpoch 147/200\n12380/12380 [==============================] - 3s 276us/step - loss: 0.6627 - accuracy: 0.7809 - val_loss: 0.6077 - val_accuracy: 0.8000\nEpoch 148/200\n12380/12380 [==============================] - 3s 273us/step - loss: 0.6564 - accuracy: 0.7834 - val_loss: 0.6051 - val_accuracy: 0.8000\nEpoch 149/200\n12380/12380 [==============================] - 3s 275us/step - loss: 0.6577 - accuracy: 0.7830 - val_loss: 0.6066 - val_accuracy: 0.8000\nEpoch 150/200\n12380/12380 [==============================] - 3s 273us/step - loss: 0.6581 - accuracy: 0.7825 - val_loss: 0.6241 - val_accuracy: 0.7987\nEpoch 151/200\n12380/12380 [==============================] - 3s 267us/step - loss: 0.6617 - accuracy: 0.7819 - val_loss: 0.6343 - val_accuracy: 0.7974\nEpoch 152/200\n12380/12380 [==============================] - 3s 278us/step - loss: 0.6594 - accuracy: 0.7821 - val_loss: 0.6053 - val_accuracy: 0.8000\nEpoch 153/200\n12380/12380 [==============================] - 3s 270us/step - loss: 0.6591 - accuracy: 0.7823 - val_loss: 0.6088 - val_accuracy: 0.8000\nEpoch 154/200\n12380/12380 [==============================] - 4s 297us/step - loss: 0.6574 - accuracy: 0.7828 - val_loss: 0.6569 - val_accuracy: 0.7625\nEpoch 155/200\n12380/12380 [==============================] - 3s 280us/step - loss: 0.6583 - accuracy: 0.7821 - val_loss: 0.6072 - val_accuracy: 0.8000\nEpoch 156/200\n12380/12380 [==============================] - 3s 281us/step - loss: 0.6584 - accuracy: 0.7826 - val_loss: 0.6452 - val_accuracy: 0.7974\nEpoch 157/200\n12380/12380 [==============================] - 4s 288us/step - loss: 0.6613 - accuracy: 0.7810 - val_loss: 0.6069 - val_accuracy: 0.8000\nEpoch 158/200\n12380/12380 [==============================] - 4s 293us/step - loss: 0.6585 - accuracy: 0.7825 - val_loss: 0.6091 - val_accuracy: 0.8000\nEpoch 159/200\n12380/12380 [==============================] - 3s 271us/step - loss: 0.6579 - accuracy: 0.7827 - val_loss: 0.6108 - val_accuracy: 0.8000\nEpoch 160/200\n12380/12380 [==============================] - 3s 279us/step - loss: 0.6619 - accuracy: 0.7818 - val_loss: 0.6081 - val_accuracy: 0.8000\nEpoch 161/200\n12380/12380 [==============================] - 3s 270us/step - loss: 0.6569 - accuracy: 0.7828 - val_loss: 0.6084 - val_accuracy: 0.8000\nEpoch 162/200\n12380/12380 [==============================] - 3s 281us/step - loss: 0.6564 - accuracy: 0.7830 - val_loss: 0.6093 - val_accuracy: 0.8000\nEpoch 163/200\n12380/12380 [==============================] - 3s 275us/step - loss: 0.6603 - accuracy: 0.7818 - val_loss: 0.6067 - val_accuracy: 0.8000\nEpoch 164/200\n12380/12380 [==============================] - 3s 268us/step - loss: 0.6584 - accuracy: 0.7823 - val_loss: 0.6118 - val_accuracy: 0.8000\nEpoch 165/200\n12380/12380 [==============================] - 4s 287us/step - loss: 0.6604 - accuracy: 0.7822 - val_loss: 0.6191 - val_accuracy: 0.8000\nEpoch 166/200\n12380/12380 [==============================] - 4s 297us/step - loss: 0.6554 - accuracy: 0.7832 - val_loss: 0.6136 - val_accuracy: 0.8000\nEpoch 167/200\n12380/12380 [==============================] - 3s 263us/step - loss: 0.6572 - accuracy: 0.7822 - val_loss: 0.6059 - val_accuracy: 0.8000\nEpoch 168/200\n12380/12380 [==============================] - 4s 312us/step - loss: 0.6637 - accuracy: 0.7813 - val_loss: 0.6122 - val_accuracy: 0.8000\nEpoch 169/200\n12380/12380 [==============================] - 3s 269us/step - loss: 0.6552 - accuracy: 0.7830 - val_loss: 0.6126 - val_accuracy: 0.8000\nEpoch 170/200\n12380/12380 [==============================] - 3s 268us/step - loss: 0.6568 - accuracy: 0.7826 - val_loss: 0.6197 - val_accuracy: 0.7990\nEpoch 171/200\n12380/12380 [==============================] - 3s 274us/step - loss: 0.6586 - accuracy: 0.7823 - val_loss: 0.6081 - val_accuracy: 0.8000\nEpoch 172/200\n12380/12380 [==============================] - 3s 273us/step - loss: 0.6588 - accuracy: 0.7826 - val_loss: 0.6117 - val_accuracy: 0.8000\nEpoch 173/200\n12380/12380 [==============================] - 3s 274us/step - loss: 0.6576 - accuracy: 0.7826 - val_loss: 0.6077 - val_accuracy: 0.8000\nEpoch 174/200\n12380/12380 [==============================] - 3s 278us/step - loss: 0.6554 - accuracy: 0.7830 - val_loss: 0.6066 - val_accuracy: 0.8000\nEpoch 175/200\n12380/12380 [==============================] - 3s 266us/step - loss: 0.6640 - accuracy: 0.7821 - val_loss: 0.6236 - val_accuracy: 0.7987\nEpoch 176/200\n12380/12380 [==============================] - 3s 278us/step - loss: 0.6584 - accuracy: 0.7821 - val_loss: 0.6099 - val_accuracy: 0.8000\nEpoch 177/200\n12380/12380 [==============================] - 3s 262us/step - loss: 0.6606 - accuracy: 0.7821 - val_loss: 0.6103 - val_accuracy: 0.8000\nEpoch 178/200\n12380/12380 [==============================] - 3s 272us/step - loss: 0.6579 - accuracy: 0.7826 - val_loss: 0.6071 - val_accuracy: 0.8000\nEpoch 179/200\n12380/12380 [==============================] - 3s 266us/step - loss: 0.6603 - accuracy: 0.7816 - val_loss: 0.6070 - val_accuracy: 0.8000\nEpoch 180/200\n12380/12380 [==============================] - 3s 270us/step - loss: 0.6571 - accuracy: 0.7826 - val_loss: 0.6453 - val_accuracy: 0.7974\nEpoch 181/200\n12380/12380 [==============================] - 3s 270us/step - loss: 0.6595 - accuracy: 0.7823 - val_loss: 0.6096 - val_accuracy: 0.8000\nEpoch 182/200\n12380/12380 [==============================] - 3s 271us/step - loss: 0.6582 - accuracy: 0.7822 - val_loss: 0.6086 - val_accuracy: 0.8000\nEpoch 183/200\n12380/12380 [==============================] - 3s 281us/step - loss: 0.6565 - accuracy: 0.7829 - val_loss: 0.6080 - val_accuracy: 0.8000\nEpoch 184/200\n12380/12380 [==============================] - 5s 369us/step - loss: 0.6569 - accuracy: 0.7829 - val_loss: 0.6093 - val_accuracy: 0.8000\nEpoch 185/200\n12380/12380 [==============================] - 4s 312us/step - loss: 0.6593 - accuracy: 0.7820 - val_loss: 0.6240 - val_accuracy: 0.7981\nEpoch 186/200\n12380/12380 [==============================] - 4s 302us/step - loss: 0.6557 - accuracy: 0.7832 - val_loss: 0.6130 - val_accuracy: 0.8000\nEpoch 187/200\n12380/12380 [==============================] - 3s 279us/step - loss: 0.6600 - accuracy: 0.7822 - val_loss: 0.6122 - val_accuracy: 0.8000\nEpoch 188/200\n12380/12380 [==============================] - 4s 300us/step - loss: 0.6597 - accuracy: 0.7820 - val_loss: 0.6073 - val_accuracy: 0.8000\nEpoch 189/200\n12380/12380 [==============================] - 4s 332us/step - loss: 0.6568 - accuracy: 0.7826 - val_loss: 0.6073 - val_accuracy: 0.8003\nEpoch 190/200\n12380/12380 [==============================] - 3s 278us/step - loss: 0.6586 - accuracy: 0.7826 - val_loss: 0.6060 - val_accuracy: 0.8000\nEpoch 191/200\n12380/12380 [==============================] - 3s 274us/step - loss: 0.6572 - accuracy: 0.7826 - val_loss: 0.6088 - val_accuracy: 0.8000\nEpoch 192/200\n12380/12380 [==============================] - 3s 268us/step - loss: 0.6577 - accuracy: 0.7822 - val_loss: 0.6062 - val_accuracy: 0.8000\nEpoch 193/200\n12380/12380 [==============================] - 3s 278us/step - loss: 0.6567 - accuracy: 0.7829 - val_loss: 0.6092 - val_accuracy: 0.8000\nEpoch 194/200\n12380/12380 [==============================] - 3s 272us/step - loss: 0.6600 - accuracy: 0.7818 - val_loss: 0.6085 - val_accuracy: 0.8000\nEpoch 195/200\n12380/12380 [==============================] - 3s 276us/step - loss: 0.6567 - accuracy: 0.7834 - val_loss: 0.6066 - val_accuracy: 0.8000\nEpoch 196/200\n12380/12380 [==============================] - 4s 292us/step - loss: 0.6587 - accuracy: 0.7823 - val_loss: 0.6108 - val_accuracy: 0.8000\nEpoch 197/200\n12380/12380 [==============================] - 3s 266us/step - loss: 0.6591 - accuracy: 0.7823 - val_loss: 0.6132 - val_accuracy: 0.8000\nEpoch 198/200\n12380/12380 [==============================] - 4s 336us/step - loss: 0.6563 - accuracy: 0.7830 - val_loss: 0.6142 - val_accuracy: 0.7997\nEpoch 199/200\n12380/12380 [==============================] - 4s 295us/step - loss: 0.6580 - accuracy: 0.7821 - val_loss: 0.6138 - val_accuracy: 0.7997\nEpoch 200/200\n12380/12380 [==============================] - 4s 295us/step - loss: 0.6586 - accuracy: 0.7827 - val_loss: 0.6090 - val_accuracy: 0.8000\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras.callbacks.callbacks.History at 0x7f13a06b8cc0>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "\n",
    "classifier.fit(x=X_train, y=y_train, validation_data=(X_test,y_test), batch_size=10, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_1_input to have shape (18,) but got array with shape (5,)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2743b0d557e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mGH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1092\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/crazy_coconut/crazy_coconut/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m         \u001b[0;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1441\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1442\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/crazy_coconut/crazy_coconut/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/crazy_coconut/crazy_coconut/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_1_input to have shape (18,) but got array with shape (5,)"
     ]
    }
   ],
   "source": [
    "GH = np.array([[5,1092,0,1,0]])\n",
    "\n",
    "y_pred = classifier.predict(GH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9489444c15ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmost_likely\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mel\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmost_likely\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mmost_likely\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "for row in y_pred:\n",
    "    most_likely = 0\n",
    "    for el in row:\n",
    "        if el > most_likely:\n",
    "            most_likely = el\n",
    "\n",
    "    print( \"rating_DB: {DB} \\nrating_EX: {EX}\\nrating_HU: {HU} \\nrating_LO: {LO} \\nrating_SD: {SD} \\nrating_SS: {SS} \\nrating_TD: {TD} \\nrating_TS: {TS} \\nrating_WV: {WV} \\n\\nMost Likely: {ml}\\n\\n\".format(DB=row[0], EX=row[1], HU=row[2], LO=row[3], SD=row[4], SS=row[5], TD=row[6], TS=row[7], WV=row[8], ml=most_likely) )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bitcrazycoconutvenv325cb191658744169d2d2744cdfa2bb1",
   "display_name": "Python 3.6.9 64-bit ('crazy_coconut': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}