{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode the dataset, only take 6:00, 12:00 and 18:00 b/c that when most \n",
    "# of the times are taken\n",
    "dataframe = pd.read_csv(\"atlantic.csv\")\n",
    "dataframe[\"time\"] = dataframe[\"time\"].astype('category')\n",
    "dataframe = pd.get_dummies(dataframe, columns=[\"time\",\"rating\",])\n",
    "\n",
    "\n",
    "\n",
    "y = dataframe[[\"rating_ DB\",\"rating_ EX\",\"rating_ HU\",\"rating_ LO\",\"rating_ SD\",\"rating_ SS\",\"rating_ TD\",\"rating_ TS\",\"rating_ WV\"]].values\n",
    "X = dataframe[[\"wind_speed\",\"pressure\", \"time_600\", \"time_1200\",\"time_1800\"]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# delete all the rows that contain no readings for pressure\n",
    "rows_to_delete = []\n",
    "row_num = 0\n",
    "for row in X:\n",
    "    if row[1] == -999:\n",
    "        rows_to_delete.append(row_num)\n",
    "    row_num += 1\n",
    "X = np.delete(X, rows_to_delete, 0)\n",
    "y = np.delete(y, rows_to_delete, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataset into training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()\n",
    "\n",
    "# input layer\n",
    "classifier.add(Dense(output_dim=6, init='uniform', activation='relu', input_dim=5))\n",
    "\n",
    "# hidden layer\n",
    "classifier.add(Dense(output_dim=6, init='uniform', activation='relu'))\n",
    "\n",
    "# output layer\n",
    "classifier.add(Dense(output_dim=9, init='uniform', activation='softmax'))\n",
    "\n",
    "# create the NN\n",
    "classifier.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From /home/devin/crazy_coconut/crazy_coconut/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n\nEpoch 1/10\n12380/12380 [==============================] - 3s 217us/step - loss: 1.2160 - accuracy: 0.5919\nEpoch 2/10\n12380/12380 [==============================] - 3s 207us/step - loss: 0.8020 - accuracy: 0.7732\nEpoch 3/10\n12380/12380 [==============================] - 2s 197us/step - loss: 0.7727 - accuracy: 0.7838\nEpoch 4/10\n12380/12380 [==============================] - 3s 214us/step - loss: 0.7634 - accuracy: 0.7830\nEpoch 5/10\n12380/12380 [==============================] - 2s 193us/step - loss: 0.7457 - accuracy: 0.7847\nEpoch 6/10\n12380/12380 [==============================] - 3s 228us/step - loss: 0.7369 - accuracy: 0.7844\nEpoch 7/10\n12380/12380 [==============================] - 2s 197us/step - loss: 0.7290 - accuracy: 0.7855\nEpoch 8/10\n12380/12380 [==============================] - 3s 229us/step - loss: 0.7223 - accuracy: 0.7863\nEpoch 9/10\n12380/12380 [==============================] - 3s 222us/step - loss: 0.7179 - accuracy: 0.7827\nEpoch 10/10\n12380/12380 [==============================] - 2s 201us/step - loss: 0.6992 - accuracy: 0.7846\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras.callbacks.callbacks.History at 0x7f1c8d5eb128>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "classifier.fit(x=X_train, y=y_train, batch_size=10, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "# y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "rating_DB: 0.008576853200793266 \nrating_EX: 0.0668913722038269\nrating_HU: 5.985428500474775e-26 \nrating_LO: 0.2972959280014038 \nrating_SD: 0.07645807415246964 \nrating_SS: 0.0004922611988149583 \nrating_TD: 0.5311508178710938 \nrating_TS: 0.007046234328299761 \nrating_WV: 0.012088434770703316 \n\nMost Likely: 0.5311508178710938\n\n\nrating_DB: 0.0005689025274477899 \nrating_EX: 0.23434194922447205\nrating_HU: 1.795838358959083e-16 \nrating_LO: 0.01000883337110281 \nrating_SD: 3.030112793567241e-06 \nrating_SS: 0.039577312767505646 \nrating_TD: 8.545827768102754e-06 \nrating_TS: 0.7149816155433655 \nrating_WV: 0.0005097754765301943 \n\nMost Likely: 0.7149816155433655\n\n\n"
    }
   ],
   "source": [
    "for row in y_pred[:2]:\n",
    "    most_likely = 0\n",
    "    for el in row:\n",
    "        if el > most_likely:\n",
    "            most_likely = el\n",
    "\n",
    "    print( \"rating_DB: {DB} \\nrating_EX: {EX}\\nrating_HU: {HU} \\nrating_LO: {LO} \\nrating_SD: {SD} \\nrating_SS: {SS} \\nrating_TD: {TD} \\nrating_TS: {TS} \\nrating_WV: {WV} \\n\\nMost Likely: {ml}\\n\\n\".format(DB=row[0], EX=row[1], HU=row[2], LO=row[3], SD=row[4], SS=row[5], TD=row[6], TS=row[7], WV=row[8], ml=most_likely) )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bitcrazycoconutvenv325cb191658744169d2d2744cdfa2bb1",
   "display_name": "Python 3.6.9 64-bit ('crazy_coconut': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}